{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import scipy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch as pt\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import manifold, neighbors, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dictionaries for labeling data from images\n",
    "def retrieveDict(nr):\n",
    "    if nr == 0:\n",
    "        dictionary = {'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,\n",
    "                   'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,\n",
    "                   'Z':25,'space':26}\n",
    "    elif nr == 1:\n",
    "        dictionary = {'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,\n",
    "                   'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,\n",
    "                   'Z':25,'space':26,'del':27,'nothing':28}\n",
    "    else:\n",
    "        print(\"No valid dictionary found\")\n",
    "        dictionary = -1\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data reading functions\n",
    "\n",
    "def readDataFolder(path, dictionary, oneHot,rescaleSize):\n",
    "    '''read data from different folders\n",
    "        data location: .../path/\n",
    "        labels assigned from dictionary default A = 1, B = 2 etc\n",
    "        oneHot determines if labels are made binary i.e. A =[1,0,...], B=[0,1,...] etc\n",
    "        returns data, rescaled to rescaleSize,rescaleSize and labels as numpy arrays\n",
    "        '''\n",
    "    data = []\n",
    "    labels = []\n",
    "    size = rescaleSize,rescaleSize\n",
    "    \n",
    "    for folder in os.listdir(path):\n",
    "        print(folder, end = ' | ')\n",
    "        for image in os.listdir(path + \"/\" + folder):\n",
    "            img = cv.imread(path + '/' + folder + '/' + image)\n",
    "            img = cv.resize(img,size)\n",
    "            data.append(img)\n",
    "            labels.append(dictionary[folder])\n",
    "    data = np.array(data)\n",
    "    data = data.astype('float32')/255.0\n",
    "    #if oneHot == 1:\n",
    "     #   labels = tf.keras.utils.to_categorical(labels)\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def readCSV(path, oneHot, rescaleSize):\n",
    "    '''read data from csv file\n",
    "        data location: .../path/\n",
    "        oneHot determines if labels are made binary i.e. A =[1,0,...], B=[0,1,...] etc or not\n",
    "        returns data, rescaled to rescaleSize,rescaleSize, and labels as numpy arrays\n",
    "        '''\n",
    "    size = rescaleSize,rescaleSize\n",
    "    data = []\n",
    "    train = pd.read_csv(path)\n",
    "    labels = train['label'].values\n",
    "    unique_val = np.array(labels)\n",
    "    np.unique(unique_val)\n",
    "    #label_binrizer = LabelBinarizer()\n",
    "    #labels = label_binrizer.fit_transform(labels)\n",
    "    train.drop('label', axis = 1, inplace = True)\n",
    "    images = train.values\n",
    "    #if oneHot == 1:\n",
    "     #   labels = tf.keras.utils.to_categorical(labels)\n",
    "        \n",
    "    for i in range(len(images)):\n",
    "        img = images[i].reshape(28,28)\n",
    "        img = cv.resize(img.astype('uint8'),size)\n",
    "        data.append(img)\n",
    "    data = np.array(data)\n",
    "    data = data.astype('float32')/255.0\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Different PreProcessing methods\n",
    "\n",
    "def preprocessing(method, data, size, parameters):\n",
    "    '''\n",
    "        Applies preprocessing to data\n",
    "            \n",
    "        '''\n",
    "    # Parameters\n",
    "    gaussianKernel = parameters[0]\n",
    "    gaussianMean = parameters[1]\n",
    "    structElement = np.ones((parameters[2],parameters[2]),np.uint8)\n",
    "    cannyKernel = parameters[3]\n",
    "    lowThreshold = parameters[4]\n",
    "    highThreshold = parameters[5]\n",
    "    \n",
    "    if method[0] == 1:\n",
    "        print(\"Not implemented, Doing Gamma Correction\")\n",
    "                       \n",
    "    if method[1] == 1:\n",
    "        print(\"Gaussian Denoising\")\n",
    "        data = gaussianDenoise(data, gaussianKernel, gaussianMean, size)\n",
    "        \n",
    "    if method[2] == 1:\n",
    "        print(\"Converting to grayscale\")\n",
    "        data = grayScale(data)\n",
    "        \n",
    "    if method[3] == 1:\n",
    "        print(\"Canny Edge detection\")\n",
    "        data = cannyEdgeDetection(data,lowThreshold,highThreshold,cannyKernel)\n",
    "        \n",
    "    if method[4] == 1:\n",
    "        print(\"Segmentation, background/foreground\")     \n",
    "        data = segmentation(data,structElement)\n",
    "        \n",
    "    if method[5] == 1:\n",
    "        print(\"Sobel Filtering\")\n",
    "        data = sobelFilter(data)\n",
    "        \n",
    "    if method[6] == 1:\n",
    "        print(\"Performing opening\")\n",
    "        data = opening(data,structElement)\n",
    "    return data\n",
    "\n",
    "def cannyEdgeDetection(inData, lowThreshold,highThreshold, kernelSize):\n",
    "    data = []\n",
    "    for i in range(len(inData)):\n",
    "        cannyEdges = cv.Canny(inData[i].astype(\"uint8\"), lowThreshold, highThreshold, kernelSize)\n",
    "        print(np.shape(cannyEdges))\n",
    "        data.append(cannyEdges)\n",
    "    data = np.array(data)\n",
    "    print(np.shape(data))\n",
    "    return\n",
    "  \n",
    "def grayScale(inData):\n",
    "    data = []\n",
    "    for i in range(len(inData)):\n",
    "        gray = cv.cvtColor(inData[i]*255, cv.COLOR_RGB2GRAY).astype(\"uint8\") #Convert to grayscale\n",
    "        data.append(gray)\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "\n",
    "        \n",
    "def gaussianDenoise(inData, kernel, mu, size):\n",
    "    data = []\n",
    "    print(np.shape(inData))\n",
    "    for i in range(len(inData)):\n",
    "        blurred = cv.GaussianBlur(inData[i], (kernel,kernel),mu)\n",
    "        data.append(blurred)\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "        \n",
    "        \n",
    "def adjustGamma(image, gamma = 1.0): # Try to implement to get working for images\n",
    "    invGamma = 1/gamma\n",
    "    table = np.array([((i/255.0)**invGamma)*255\n",
    "        for i in np.arange(0,256)]).astype(\"uint8\")\n",
    "    return cv.LUT(image,table)\n",
    "\n",
    "def segmentation(inData,structElement,grayed):\n",
    "    data = []\n",
    "    for i in range(len(inData)): \n",
    "        if grayed == 0: \n",
    "            gray = cv.cvtColor(inData[i]*255, cv.COLOR_RGB2GRAY).astype(\"uint8\") #Convert to grayscale\n",
    "        elif grayed == 1:\n",
    "            gray = inData[i]\n",
    "            \n",
    "        thresholding1 = cv.adaptiveThreshold(gray,255,cv.ADAPTIVE_THRESH_MEAN_C,cv.THRESH_BINARY,11,2) #perform adaptive thresholing\n",
    "        opening = cv.morphologyEx(thresholding1 , cv.MORPH_OPEN, structElement, iterations=2) #perform opening\n",
    "        distance_transform = cv.distanceTransform(opening, cv.DIST_L2, 5)\n",
    "        data.append(distance_transform)\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "\n",
    "def opening(inData,structElement):\n",
    "    data = []\n",
    "    for i in range(len(inData)):\n",
    "        opening = cv.morphologyEx(inData[i] , cv.MORPH_OPEN, structElement, iterations=2) #perform opening\n",
    "        data.append(opening)\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "\n",
    "def sobelFilter(inData):\n",
    "    data = []\n",
    "    for i in range(len(inData)):\n",
    "        sobel = cv.Sobel(inData[i],cv.CV_64F,0,1,ksize=3)\n",
    "        data.append(sobel)\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "\n",
    "def splitData(data, labels, testSize, binaryLabel):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size = 0.3, stratify = labels, random_state = 7)\n",
    "    \n",
    "    if binaryLabel == 1: #Need to binarize data after splitting, if data is binarized QDA and LDA won't work\n",
    "        y_train = tf.keras.utils.to_categorical(y_train)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize things\n",
    "\n",
    "def plotSamples(data, samples, size):\n",
    "    fig, axs = plt.subplots(1, samples, figsize=(15, 4), sharey=True)\n",
    "    for i in range(samples): \n",
    "        axs[i].imshow(data[np.random.randint(len(data),size = 1)].reshape(size,size))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Augmentation\n",
    "\n",
    "def randomNoise(origData, origLabel, runs, strength):\n",
    "    \"\"\"\n",
    "    Augments the data by naively aplying random gaussian noise to array, applies same strength to whole array\n",
    "        :param original data, original labels, how many times to augment data, the strength of the noise\n",
    "        :return: augmented list of data and labels\n",
    "        \"\"\"\n",
    "    labelToAppend = origLabel\n",
    "    dataToPermutate = origData\n",
    "    for i in range(runs): #For each augmentation\n",
    "        origLabel = np.vstack((origLabel,labelToAppend)) #Extend labels since wwe don't want to change them at all\n",
    "        origData = np.vstack((origData, plusNoise(dataToPermutate, strength))) #Add data where noise have been added\n",
    "        origLabel = np.vstack((origLabel,labelToAppend)) #Extend labels again\n",
    "        origData = np.vstack((origData, negNoise(dataToPermutate, strength))) #Add data where noise have been subtracted\n",
    "    return origData, origLabel\n",
    "\n",
    "def plusNoise(inData, strength):\n",
    "    \"\"\"\n",
    "    Add gausian white noise to data\n",
    "        :param Data to apply noise to, strength of noise to apply\n",
    "        :return: Data where noise have been added\n",
    "        \"\"\"\n",
    "    data = inData\n",
    "    for i in range(len(inData)): # For each element\n",
    "        data[i] = inData[i]+np.random.normal(0, strength, size=(len(data[0]))) #Add noise\n",
    "    return data\n",
    "\n",
    "def negNoise(inData, strength):\n",
    "    \"\"\"\n",
    "    Subtract gaussian white noise from data\n",
    "        :param Data to subtract from, strength of noise\n",
    "        :return: Data where noise have been subtracted\n",
    "        \"\"\"\n",
    "    data = inData\n",
    "    for i in range(len(inData)): # for each element\n",
    "        data[i] = inData[i] + np.random.normal(0, strength, size=(len(data[0]))) # Remove noise\n",
    "    return data\n",
    "\n",
    "def rotate(data): #To be written\n",
    "    return data\n",
    "def flip(data): #To be written\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Different non deep learning classifiers\n",
    "\n",
    "def trainNonDeepLearning(models, modelParameters, data, labels):\n",
    "    '''\n",
    "    Train a selection of models and return them\n",
    "    '''\n",
    "    randomForestDepth = modelParameters[0]\n",
    "    trainedModels = []\n",
    "    if models[0] == 1:\n",
    "        model = decisionTree(data, labels)\n",
    "    elif models[1] == 1:\n",
    "        model = randomForest(data, labels, randomForestDepth)\n",
    "    elif models[2] == 1:\n",
    "        model = knn(data, labels)\n",
    "    elif models[3] == 1: \n",
    "        model = qda(data, labels)\n",
    "    elif models[4] == 1:\n",
    "        model = lda(data, labels)\n",
    "    elif models[5] == 1:\n",
    "        model = svm(data, labels)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def decisionTree(data, labels):\n",
    "    '''\n",
    "    Decision tree classifier\n",
    "    '''\n",
    "    model = tree.DecisionTreeClassifier()#Create model\n",
    "    print(\"Fitting data to decision tree.\")\n",
    "    model.fit(data,labels) #Train model\n",
    "    return model\n",
    "\n",
    "def randomForest(data, labels, depth):\n",
    "    '''\n",
    "    Random forest classifier\n",
    "    '''\n",
    "    model = RandomForestClassifier(max_depth = depth)#Create model\n",
    "    print(\"Fitting data to random forest.\")\n",
    "    model.fit(data,labels) #Train model\n",
    "    return model\n",
    "\n",
    "def knn(data, labels): #Try to change neighbors to not be hardcoded in function call\n",
    "    '''\n",
    "    k-nearet neighbors classifier\n",
    "    '''\n",
    "    model = neighbors.KNeighborsClassifier(5, weights='distance')\n",
    "    print(\"Fitting data to K-NN model\")\n",
    "    model.fit(data, labels)\n",
    "    return model\n",
    "\n",
    "def qda(data, labels):\n",
    "    \"\"\"\n",
    "    Trains a qda model from data and labels\n",
    "        :param data and labels to train with\n",
    "        :return: trained qda model\n",
    "        \"\"\"\n",
    "    model = QuadraticDiscriminantAnalysis() #Create classifier\n",
    "    print(\"Fitting data to quadratic discriminant model\")\n",
    "    model.fit(data, labels) # train classifier\n",
    "    return model\n",
    "\n",
    "def lda(data, labels):\n",
    "    \"\"\"\n",
    "        Trains a lda model from data and labels\n",
    "            :param data and labels to train with\n",
    "            :return: trained lda model\n",
    "            \"\"\"\n",
    "    model = LinearDiscriminantAnalysis() #Create model\n",
    "    print(\"Fitting data to linear discriminant model\")\n",
    "    model.fit(data,labels) #Train model\n",
    "    return model\n",
    "\n",
    "def svm(data,labels):\n",
    "    \"\"\"\n",
    "        Trains a svm model from data and labels\n",
    "            :param data and labels to train with\n",
    "            :return: trained svm model\n",
    "            \"\"\"\n",
    "    model = LinearSVC(max_iter= 100000) #Increase to help with badly conditioned data\n",
    "    model = CalibratedClassifierCV(model)\n",
    "    print(\"Fitting data to support vector\")\n",
    "    model.fit(data,labels) #Train svm classifier\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation methods\n",
    "\n",
    "def evaluateSKlearn(model,data,labels):\n",
    "    print(\"Evaluation model.\")\n",
    "    prediction = model.predict(data)\n",
    "    accuracy = metrics.accuracy_score(labels,prediction)\n",
    "    \n",
    "    print(\"\\n Accuracy: %f\",accuracy)\n",
    "    print(\"\\n Results from classifier:  \\n %s \\n\" %( metrics.classification_report(labels, prediction)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''Main method for readability of code'''\n",
    "    ## Parameters\n",
    "    dataset = 2 # 0 for significant ASL, 1 for ASL dataset, 2 for MNIST dataset\n",
    "    binaryLabel = 0 # 0 for encoded i.e. A=1, B=2 etc 1 to one hot encode data i.e. A =[1,0,...], B = [0,1,...] etc\n",
    "    rescaleSize = 64 # rescale images to size rescaleSize,rescaleSize \n",
    "    currentLocation = os.getcwd()\n",
    "    vissualize = 0\n",
    "    testSize = 0.3 #PErcentage of training data kept for testing\n",
    "    \n",
    "    ## preprocess methods\n",
    "    gammaCorrection = 0 # Do gamma correction to move average pixel intesity to range 100-150 Not implemented yet\n",
    "    gaussianBlur = 0 # If we apply gaussian blur to images\n",
    "    grayScale = 0 #Convert to grayscale automatically done if using segmentation\n",
    "    edgeDetection = 0 # If we do edge detection, using canny edge detector\n",
    "    segmentation = 0 # Try to segment image into foreground and background \n",
    "    sobelFilter = 0 # Try to segment image into different objects\n",
    "    opening = 0 # Do morphological opening, erosion followed by dilation\n",
    "    \n",
    "    methods = []\n",
    "    methods.append(gammaCorrection)\n",
    "    methods.append(gaussianBlur)\n",
    "    methods.append(grayScale)\n",
    "    methods.append(edgeDetection)\n",
    "    methods.append(segmentation)\n",
    "    methods.append(sobelFilter)\n",
    "    methods.append(opening)\n",
    "    \n",
    "    # preprocessing parameters\n",
    "    gaussianKernel = 5 #Size of gaussian kernel low pass filter\n",
    "    gaussianMean = 0 #Mean of gaussian low pass filter\n",
    "    structElementKernel = 3 #Size of structuring element used in opening, erosion etc. \n",
    "    cannyKernel = 3 #Kernel for canny edge detection\n",
    "    lowThreshold = 40 #Low threshold for canny edge detection\n",
    "    highThreshold = 120 #High threshold for canny edge detection\n",
    "    \n",
    "    preprocessingParameters = []\n",
    "    preprocessingParameters.append(gaussianKernel)\n",
    "    preprocessingParameters.append(gaussianMean)\n",
    "    preprocessingParameters.append(structElementKernel)\n",
    "    preprocessingParameters.append(cannyKernel)\n",
    "    preprocessingParameters.append(lowThreshold)\n",
    "    preprocessingParameters.append(highThreshold)\n",
    "    \n",
    "    \n",
    "    ## non deep learning classifiers, can only train one model at a time\n",
    "    decissionTree = 0\n",
    "    randomForest = 1\n",
    "    kNearestNeighbor = 0\n",
    "    quadraticDiscriminantAnalysis = 0\n",
    "    linearDiscriminantAnalysis = 0\n",
    "    supportVectorClassification = 0 #Slow training\n",
    "    \n",
    "    classifiers = []\n",
    "    classifiers.append(decissionTree)\n",
    "    classifiers.append(randomForest)\n",
    "    classifiers.append(kNearestNeighbor)\n",
    "    classifiers.append(quadraticDiscriminantAnalysis)\n",
    "    classifiers.append(linearDiscriminantAnalysis)\n",
    "    classifiers.append(supportVectorClassification)\n",
    "    \n",
    "    if sum(classifiers) != 1:\n",
    "        print(\"Incorrect number of classifiers selected, program supports training 1 classifier at a time.\")\n",
    "        sys.exit()\n",
    "        \n",
    "    if binaryLabel == 1 and (quadraticDiscriminantAnalysis == 1 or linearDiscriminantAnalysis ==1):\n",
    "        print(\"Warning: QDA and LDA does not work with binary labels setting labels to non binary\")\n",
    "        binaryLabel = 0\n",
    "    #non deep learning classifier parameters\n",
    "    randomForestMaxDepth = 12\n",
    "    \n",
    "    classifierParameters = []\n",
    "    classifierParameters.append(randomForestMaxDepth)\n",
    "\n",
    "    \n",
    "    \n",
    "    ## Load data\n",
    "    if dataset == 0:\n",
    "        path = currentLocation +'/significant-asl-sign-language-alphabet-dataset/Training-Set'\n",
    "    elif dataset == 1:\n",
    "        path = currentLocation + '/asl-alphabet/asl_alphabet_train/asl_alphabet_train' \n",
    "    elif dataset == 2:\n",
    "        path = currentLocation + '/data/sign_mnist_train.csv' \n",
    "    else:\n",
    "        print('Invalid dataset')\n",
    "        exit()\n",
    "    \n",
    "    dictionary = retrieveDict(dataset)\n",
    "    print('Reading data.')\n",
    "    if dataset == 0 or dataset ==  1:\n",
    "        data, labels = readDataFolder(path, dictionary, binaryLabel, rescaleSize)\n",
    "    elif dataset == 2:\n",
    "        data, labels = readCSV(path,binaryLabel, rescaleSize)\n",
    "    \n",
    "    # Visualize some samples of data\n",
    "    if vissualize == 1:\n",
    "        print('Visualize 5 samples from data.')\n",
    "        plotSamples(data,5,rescaleSize)\n",
    "    \n",
    "    \n",
    "    ## Preprocessing\n",
    "    data = preprocessing(methods, data, rescaleSize, preprocessingParameters)\n",
    "    \n",
    "    ## Reshape data into size(nr_samples,features)\n",
    "    if dataset == 0 or dataset ==  1:\n",
    "        data = data.reshape(-1,(rescaleSize*rescaleSize))  \n",
    "    elif dataset == 2:\n",
    "        data = data.reshape(-1,(rescaleSize*rescaleSize))  \n",
    "    # Split data into train and test data and convert labels to binary if desired\n",
    "    trainData, testData, trainLabel, testLabel = splitData(data, labels, testSize, binaryLabel)\n",
    "    \n",
    "    ## Train and evaluate non deep learning model\n",
    "    model = trainNonDeepLearning(classifiers, classifierParameters, trainData, trainLabel)\n",
    "    evaluateSKlearn(model,testData,testLabel)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

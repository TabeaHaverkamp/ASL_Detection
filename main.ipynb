{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import scipy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import torch as pt\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import manifold, neighbors, metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, BatchNormalization, AveragePooling2D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from joblib import dump, load\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dictionaries for labeling data from images\n",
    "def retrieveDict(nr):\n",
    "    if nr == 0:\n",
    "        dictionary = {'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,\n",
    "                   'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,\n",
    "                   'Z':25,'space':26}\n",
    "    elif nr == 1:\n",
    "        dictionary = {'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9,'K':10,'L':11,'M':12,\n",
    "                   'N':13,'O':14,'P':15,'Q':16,'R':17,'S':18,'T':19,'U':20,'V':21,'W':22,'X':23,'Y':24,\n",
    "                   'Z':25,'space':26,'del':27,'nothing':28}\n",
    "    else:\n",
    "        print(\"No valid dictionary found\")\n",
    "        dictionary = -1\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data reading functions\n",
    "\n",
    "def readDataFolder(path, dictionary, oneHot,rescaleSize):\n",
    "    '''read data from different folders\n",
    "        data location: .../path/\n",
    "        labels assigned from dictionary default A = 1, B = 2 etc\n",
    "        oneHot determines if labels are made binary i.e. A =[1,0,...], B=[0,1,...] etc\n",
    "        returns data, rescaled to rescaleSize,rescaleSize and labels as numpy arrays\n",
    "        '''\n",
    "    data = []\n",
    "    labels = []\n",
    "    size = rescaleSize,rescaleSize\n",
    "    #folder ='A'\n",
    "    for folder in os.listdir(path):\n",
    "        print(folder, end = ' | ')\n",
    "        for image in os.listdir(path + \"/\" + folder):\n",
    "            img = cv.imread(path + '/' + folder + '/' + image)\n",
    "            img = cv.resize(img,size)\n",
    "            data.append(img)\n",
    "            labels.append(dictionary[folder])\n",
    "    data = np.array(data)\n",
    "    data = data.astype('float32')/255.0\n",
    "\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "def readCSV(path, rescaleSize):\n",
    "    '''read data from csv file\n",
    "        data location: .../path/\n",
    "        returns data, rescaled to rescaleSize,rescaleSize, and labels as numpy arrays\n",
    "        '''\n",
    "    size = rescaleSize,rescaleSize\n",
    "    data = []\n",
    "    train = pd.read_csv(path)\n",
    "    labels = train['label'].values\n",
    "    unique_val = np.array(labels)\n",
    "    np.unique(unique_val)\n",
    "    \n",
    "    train.drop('label', axis = 1, inplace = True)\n",
    "    images = train.values\n",
    " \n",
    "    for i in range(len(images)):\n",
    "        img = images[i].reshape(28,28)\n",
    "        img = cv.resize(img.astype('uint8'),size)\n",
    "        data.append(img)\n",
    "    data = np.array(data)\n",
    "    data = data.astype('float32')/255.0\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Different PreProcessing methods\n",
    "\n",
    "def preprocessing(method, data, size, parameters, dataset):\n",
    "    '''\n",
    "        Applies preprocessing to data\n",
    "            \n",
    "        '''\n",
    "    # Parameters\n",
    "    gaussianKernel = parameters[0]\n",
    "    gaussianMean = parameters[1]\n",
    "    structElement = np.ones((parameters[2],parameters[2]),np.uint8)\n",
    "    cannyKernel = parameters[3]\n",
    "    lowThreshold = parameters[4]\n",
    "    highThreshold = parameters[5]\n",
    "    \n",
    "    if method[0] == 1:\n",
    "        print(\"Not implemented, Doing Gamma Correction\")\n",
    "                       \n",
    "    if method[1] == 1:\n",
    "        print(\"Gaussian Denoising\")\n",
    "        data = gaussianDenoise(data, gaussianKernel, gaussianMean, size)\n",
    "        \n",
    "    if method[2] == 1:\n",
    "        print(\"Converting to grayscale\")\n",
    "        data = grayScale(data)\n",
    "        \n",
    "    if method[3] == 1:\n",
    "        print(\"Canny Edge detection\")\n",
    "        data = cannyEdgeDetection(data,lowThreshold,highThreshold,cannyKernel)\n",
    "        \n",
    "    if method[4] == 1:\n",
    "        print(\"Segmentation, background/foreground\")     \n",
    "        data = segmentation(data,structElement, method[2], dataset)\n",
    "        \n",
    "    if method[5] == 1:\n",
    "        print(\"Sobel Filtering\")\n",
    "        data = sobelFilter(data)\n",
    "        \n",
    "    if method[6] == 1:\n",
    "        print(\"Performing opening\")\n",
    "        data = opening(data,structElement)\n",
    "    return data\n",
    "\n",
    "def cannyEdgeDetection(inData, lowThreshold,highThreshold, kernelSize):\n",
    "    data = []\n",
    "    for i in range(len(inData)):\n",
    "        cannyEdges = cv.Canny(inData[i].astype(\"uint8\"), lowThreshold, highThreshold, kernelSize)\n",
    "        data.append(cannyEdges)\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "  \n",
    "def grayscale(inData):\n",
    "    data = []\n",
    "    for i in range(len(inData)):\n",
    "        gray = cv.cvtColor(inData[i]*255, cv.COLOR_RGB2GRAY).astype(\"uint8\") #Convert to grayscale\n",
    "        data.append(gray)\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "\n",
    "        \n",
    "def gaussianDenoise(inData, kernel, mu, size):\n",
    "    data = []\n",
    "    for i in range(len(inData)):\n",
    "        blurred = cv.GaussianBlur(inData[i], (kernel,kernel),mu)\n",
    "        data.append(blurred)\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "        \n",
    "def adjustGamma(image, gamma = 1.0): # Try to implement to get working for images\n",
    "    invGamma = 1/gamma\n",
    "    table = np.array([((i/255.0)**invGamma)*255\n",
    "        for i in np.arange(0,256)]).astype(\"uint8\")\n",
    "    return cv.LUT(image,table)\n",
    "\n",
    "def segmentation(inData,structElement,grayed,dataset):\n",
    "    data = []\n",
    "    for i in range(len(inData)): \n",
    "        if grayed == 0 and dataset != 2: \n",
    "            gray = cv.cvtColor(inData[i]*255, cv.COLOR_RGB2GRAY).astype(\"uint8\") #Convert to grayscale\n",
    "        elif grayed == 1 or dataset == 2:\n",
    "            gray = inData[i]\n",
    "            \n",
    "        thresholding1 = cv.adaptiveThreshold(gray.astype('uint8'),255,cv.ADAPTIVE_THRESH_MEAN_C,cv.THRESH_BINARY,11,2) #perform adaptive thresholing\n",
    "        opening = cv.morphologyEx(thresholding1 , cv.MORPH_OPEN, structElement, iterations=2) #perform opening\n",
    "        distance_transform = cv.distanceTransform(opening, cv.DIST_L2, 5)\n",
    "        data.append(distance_transform)\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "\n",
    "def opening(inData,structElement):\n",
    "    data = []\n",
    "    for i in range(len(inData)):\n",
    "        opening = cv.morphologyEx(inData[i] , cv.MORPH_OPEN, structElement, iterations=2) #perform opening\n",
    "        data.append(opening)\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "\n",
    "def sobelFilter(inData):\n",
    "    data = []\n",
    "    for i in range(len(inData)):\n",
    "        sobel = cv.Sobel(inData[i],cv.CV_64F,0,1,ksize=3)\n",
    "        data.append(sobel)\n",
    "    data = np.array(data)\n",
    "    return data\n",
    "\n",
    "def splitData(data, labels, testSize, binaryLabel, pca):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size = 0.3, stratify = labels, random_state = 7)\n",
    "    \n",
    "    if binaryLabel == 1: #Need to binarize data after splitting, if data is binarized QDA and LDA won't work\n",
    "        y_train = tf.keras.utils.to_categorical(y_train)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test)\n",
    "    \n",
    "    if pca == 1:\n",
    "        print(\"Performing PCA\")\n",
    "        pca = PCA(0.95)\n",
    "        pca.fit(x_train)\n",
    "        x_train = pca.transform(x_train)\n",
    "        x_test = pca.transform(x_test)\n",
    "        \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize things\n",
    "\n",
    "def plotSamples(data, samples, size):\n",
    "    fig, axs = plt.subplots(1, samples, figsize=(15, 4), sharey=True)\n",
    "    if len(np.shape(data)) == 4:\n",
    "        for i in range(samples): \n",
    "            axs[i].imshow(data[i])\n",
    "    else:\n",
    "        for i in range(samples): \n",
    "            axs[i].imshow(data[np.random.randint(len(data),size = 1)].reshape(size,size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Augmentation\n",
    "\n",
    "def augmentData(data, labels, parameters, methods):\n",
    "    runs = parameters[0] #Nr of times to multiply data i.e. 1 leads to twice the original data 2=3 times etc.\n",
    "    strength = parameters[1] #Strength of gaussian noise\n",
    "    maxDegree = parameters[2] #Maximum degree to rotate image\n",
    "    size = parameters[3]\n",
    "    labelToAppend = labels\n",
    "    \n",
    "    if methods[1] == 1:\n",
    "        augmentedData = data.reshape(len(data),size,size) #Rotate requires shape of (a,b,c)\n",
    "        origData = data.reshape(len(data),size,size)\n",
    "    else:\n",
    "        augmentedData = data\n",
    "        origData = data\n",
    "    dataLength = len(data)\n",
    "\n",
    "    for i in range(runs):\n",
    "        for j in range(dataLength):\n",
    "            if methods[0] == 1:\n",
    "                augmentedData[j] = randomNoise(origData[j],strength)\n",
    "            if methods[1] == 1:\n",
    "                augmentedData[j] = rotate(origData[j],maxDegree)\n",
    "            if methods[2] == 1:\n",
    "                augmentedData[j] = flip(origData[j])\n",
    "        labels = np.hstack((labels,labelToAppend)) #Extend labels since we don't want to change them at all\n",
    "        origData = np.vstack((origData, augmentedData))\n",
    "    data = origData.reshape(-1,(size*size))\n",
    "    ## Shuffle data at end\n",
    "    return data, labels\n",
    "\n",
    "def randomNoise(data, strength):\n",
    "    \"\"\"\n",
    "    Subtract gaussian white noise from data\n",
    "        :param Data to subtract from, strength of noise\n",
    "        :return: Data where noise have been subtracted\n",
    "        \"\"\"\n",
    "    randNoise = np.random.rand()\n",
    "    if randNoise < 0.5:\n",
    "        data = np.subtract(data,np.random.normal(0, strength, size=np.shape(data))) # Subtract noise\n",
    "    else:\n",
    "        data = np.add(data,np.random.normal(0, strength, size=np.shape(data))) # Subtract noise\n",
    "\n",
    "    return data\n",
    "\n",
    "def rotate(data, maxAngle): #To be written\n",
    "    \n",
    "    center = (64 / 2, 64 / 2)\n",
    "    angle = np.random.uniform(0,maxAngle,1)\n",
    "    direction = np.random.rand()\n",
    "    if direction < 0.5:\n",
    "        angle = -angle\n",
    "    rotationMatrix = cv.getRotationMatrix2D(center,angle,1.0)\n",
    "    data = cv.warpAffine(data, rotationMatrix, (64,64))\n",
    "    return data\n",
    "\n",
    "def flip(data): \n",
    "    axis = np.random.randint(3,size = 1)\n",
    "    if axis == 0:\n",
    "        data = cv.flip(data, 0) #Vertical\n",
    "    elif axis == 1:\n",
    "        data = cv.flip(data,1) #Horizontal\n",
    "    else:\n",
    "        data = cv.flip(data,-1) #Horizontal and vertical\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Different non deep learning classifiers\n",
    "\n",
    "def trainNonDeepLearning(models, modelParameters, data, labels):\n",
    "    '''\n",
    "    Train a selection of models and return them\n",
    "    '''\n",
    "    randomForestDepth = modelParameters[0]\n",
    "    trainedModels = []\n",
    "    if models[0] == 1:\n",
    "        model = decisionTree(data, labels)\n",
    "    elif models[1] == 1:\n",
    "        model = randomForest(data, labels, randomForestDepth)\n",
    "    elif models[2] == 1:\n",
    "        model = knn(data, labels)\n",
    "    elif models[3] == 1: \n",
    "        model = qda(data, labels)\n",
    "    elif models[4] == 1:\n",
    "        model = lda(data, labels)\n",
    "    elif models[5] == 1:\n",
    "        model = svm(data, labels)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def decisionTree(data, labels):\n",
    "    '''\n",
    "    Decision tree classifier\n",
    "    '''\n",
    "    model = tree.DecisionTreeClassifier()#Create model\n",
    "    print(\"Fitting data to decision tree.\")\n",
    "    model.fit(data,labels) #Train model\n",
    "    dump(model, 'decisionTree.joblib')\n",
    "\n",
    "    return model\n",
    "\n",
    "def randomForest(data, labels, depth):\n",
    "    '''\n",
    "    Random forest classifier\n",
    "    '''\n",
    "    model = RandomForestClassifier(max_depth = depth)#Create model\n",
    "    print(\"Fitting data to random forest.\")\n",
    "    model.fit(data,labels) #Train model\n",
    "    dump(model, 'randomForest.joblib')\n",
    "\n",
    "    return model\n",
    "\n",
    "def knn(data, labels): #Try to change neighbors to not be hardcoded in function call\n",
    "    '''\n",
    "    k-nearet neighbors classifier\n",
    "    '''\n",
    "    model = neighbors.KNeighborsClassifier(5, weights='distance')\n",
    "    print(\"Fitting data to K-NN model\")\n",
    "    model.fit(data, labels)\n",
    "    dump(model, 'knn.joblib')\n",
    "    return model\n",
    "\n",
    "def qda(data, labels):\n",
    "    \"\"\"\n",
    "    Trains a qda model from data and labels\n",
    "        :param data and labels to train with\n",
    "        :return: trained qda model\n",
    "        \"\"\"\n",
    "    model = QuadraticDiscriminantAnalysis() #Create classifier\n",
    "    print(\"Fitting data to quadratic discriminant model\")\n",
    "    model.fit(data, labels) # train classifier\n",
    "    dump(model, 'qda.joblib')\n",
    "    return model\n",
    "\n",
    "def lda(data, labels):\n",
    "    \"\"\"\n",
    "        Trains a lda model from data and labels\n",
    "            :param data and labels to train with\n",
    "            :return: trained lda model\n",
    "            \"\"\"\n",
    "    model = LinearDiscriminantAnalysis() #Create model\n",
    "    print(\"Fitting data to linear discriminant model\")\n",
    "    model.fit(data,labels) #Train model\n",
    "    dump(model, 'lda.joblib')\n",
    "    return model\n",
    "\n",
    "def svm(data,labels):\n",
    "    \"\"\"\n",
    "        Trains a svm model from data and labels\n",
    "            :param data and labels to train with\n",
    "            :return: trained svm model\n",
    "            \"\"\"\n",
    "    model = LinearSVC(max_iter= 100000) #Increase to help with badly conditioned data\n",
    "    model = CalibratedClassifierCV(model)\n",
    "    print(\"Fitting data to support vector\")\n",
    "    model.fit(data,labels) #Train svm classifier\n",
    "    dump(model, 'SVM.joblib')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Evaluation methods\n",
    "\n",
    "def evaluateSKlearn(model, data, labels, kfold, visualize):\n",
    "    print(\"Evaluation model.\")\n",
    "    prediction = model.predict(data)\n",
    "    accuracy = metrics.accuracy_score(labels,prediction)\n",
    "    \n",
    "    print(\"\\n Accuracy: %f\",accuracy)\n",
    "    print(\"\\n Results from classifier:  \\n %s \\n\" %( metrics.classification_report(labels, prediction)))\n",
    "    if visualize == 1: #Since confusion matrix takes a lot of space\n",
    "        print(\"\\n Confussion matrix:\\n %s\" % metrics.confusion_matrix(labels, prediction)) #Print confusion matrix\n",
    "\n",
    "        \n",
    "    #Perform kfold cross validation\n",
    "    scores = cross_val_score(model, data, labels, cv=kfold) #Do cross validation\n",
    "    print(\"\\n Cross validation score: \\n\")\n",
    "    print(scores) #Report scores of cross validation\n",
    "    print(\"\\n Mean cv score: %f\", np.mean(scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN Architectures\n",
    "\n",
    "# From paper Using Deep Convolutional Networks for Gesture Recognition in American Sign Language\n",
    "\n",
    "def bhedamNet(size,channels,classes,opti):\n",
    "    '''\n",
    "    Number of filters for each layer are not specified in paper \n",
    "    \n",
    "    '''\n",
    "    inputDim = (size,size,channels)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation='relu', input_shape= inputDim)) #Input layer \n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation='relu', padding = 'same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation='relu', padding = 'same'))\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation='relu', padding = 'same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation='relu', padding = 'same'))\n",
    "    model.add(Conv2D(16, kernel_size = (3,3), activation='relu', padding = 'same'))\n",
    "    model.add(MaxPooling2D(pool_size = (2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "        \n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))   \n",
    "        \n",
    "    model.add(Dense(classes, activation = 'softmax')) #Output layer\n",
    "\n",
    "    model.compile(optimizer = opti, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "    \n",
    "# From a kaggle user\n",
    "def kaggleNet(size,channels, classes, opti):\n",
    "    \n",
    "    inputDim = (size,size,channels)\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(64, kernel_size = 4, strides = 1, activation = 'relu', input_shape=inputDim)) #input layer\n",
    "    model.add(Conv2D(64, kernel_size = 4, strides = 2, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size = 4, strides = 1, activation = 'relu'))\n",
    "    model.add(Conv2D(128, kernel_size = 4, strides = 2, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size = 4, strides = 1, activation = 'relu'))\n",
    "    model.add(Conv2D(256, kernel_size = 4, strides = 2, activation = 'relu'))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(512, activation = 'relu'))\n",
    "    model.add(Dense(classes, activation = 'softmax')) #output layer\n",
    "\n",
    "    model.compile(optimizer = opti, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "        \n",
    "# From paper Recognition Bangla Sign Language using Convolutional Neural Network\n",
    "def islalmNet(size,channels, classes, opti):\n",
    "    inputDim = (size,size,channels)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size = (5,5), padding = 'same', input_shape= inputDim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size(2,2)))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = (5,5), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(256, kernel_size = (3,3), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(384, kernel_size = (3,3), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size(2,2)))\n",
    "    \n",
    "    model.add(Conv2D(512, kernel_size = (3,3), padding = 'same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size(2,2)))\n",
    "    \n",
    "    model.add(AveragePooling2D(2,2))\n",
    "    model.add(Dense(84, activation = 'relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(classes, activation ='softmax'))\n",
    "    \n",
    "    model.compile(optimizer = opti, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train and test neural network\n",
    "def trainCNN(size, data, labels, architecture, parameter):\n",
    "    '''\n",
    "    Train a cnn using one of three architectures \n",
    "    '''\n",
    "    channels = np.shape(data)[3]\n",
    "    \n",
    "    callback = [EarlyStopping(monitor='val_loss', patience=parameter[0]), \n",
    "                ModelCheckpoint(filepath='BestModel.hdf5', monitor='val_loss', save_best_only=True)] #Set up callback, early stopping regularisation\n",
    "    \n",
    "    classes = np.amax(labels)+1\n",
    "\n",
    "    optimizer = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False) #Set up Adam optimiser\n",
    "            \n",
    "    trainingData, validationData, trainingLabels, validationLabels = train_test_split(data, labels, test_size = 0.3, stratify = labels, random_state = 7)\n",
    "\n",
    "    if architecture[0] == 1:\n",
    "        model = bhedamNet(size, channels, classes, optimizer)\n",
    "    elif architecture[1] == 1:\n",
    "        model = kaggleNet(size, channels, classes, optimizer)\n",
    "    elif architecture[2] == 1:\n",
    "        model = islalmNet(size, channels, classes, optimizer)\n",
    "        \n",
    "    if parameter[3] == 1:\n",
    "        print(\"Using real-time data augmentation\")\n",
    "        augmentation = ImageDataGenerator(\n",
    "                        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                        height_shift_range=0.1, # randomly shift images vertically (fraction of total height)\n",
    "                        shear_range = 0.1,  # set range for random shear\n",
    "                        zoom_range = 0.1,  # set range for random zoom\n",
    "                        fill_mode = 'nearest', # set mode for filling points outside the input boundaries\n",
    "                        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "                        horizontal_flip=True,  # randomly flip images\n",
    "                        vertical_flip=True,  # randomly flip images\n",
    "                        rescale=None,       # set rescaling factor (applied before any other transformation)\n",
    "                        validation_split = 0.1)  # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        augmentation.fit(trainingData)\n",
    "        trainedModel = model.fit(augmentation.flow(trainingData, trainingLabels, batch_size = parameter[1]),\n",
    "                                    epochs = parameter[2], callbacks=callback, verbose = 1, \n",
    "                                    validation_data =(validationData, validationLabels))\n",
    "        trainedModel = load_model('weights-best.hdf5')\n",
    "    \n",
    "    else:\n",
    "        print(\"Not using data augmentation\")   \n",
    "        trainedModel = model.fit(trainingData, trainingLabels, batch_size = parameter[1], \n",
    "                                 epochs = parameter[2], callbacks=callback, verbose = 1, \n",
    "                                 validation_data=(validationData, validationLabels), shuffle = True) \n",
    "        \n",
    "        trainedModel = load_model('weights-best.hdf5')\n",
    "    \n",
    "    return trainedModel\n",
    "\n",
    "def evaluateCNN(testData,testLabels, model):\n",
    "    print(\"Evaluation network.\")\n",
    "    scores = model.evaluate(testData, testLabels, verbose=1)\n",
    "    print('Test loss:', scores[0])\n",
    "    print('Test accuracy:', scores[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No valid dictionary found\n",
      "Reading data.\n"
     ]
    }
   ],
   "source": [
    "### Read data\n",
    "dataset = 2 # 0 for significant ASL, 1 for ASL dataset, 2 for MNIST dataset\n",
    "currentLocation = os.getcwd()\n",
    "rescaleSize = 64 # rescale images to size rescaleSize,rescaleSize \n",
    "binaryLabel = 0 # 0 for encoded i.e. A=1, B=2 etc 1 to one hot encode data i.e. A =[1,0,...], B = [0,1,...] etc\n",
    "\n",
    "## Load data\n",
    "if dataset == 0:\n",
    "    path = currentLocation +'/significant-asl-sign-language-alphabet-dataset/Training-Set'\n",
    "elif dataset == 1:\n",
    "    path = currentLocation + '/asl-alphabet/asl_alphabet_train/asl_alphabet_train' \n",
    "elif dataset == 2:\n",
    "    path = currentLocation + '/data/sign_mnist_train.csv' \n",
    "else:\n",
    "    print('Invalid dataset')\n",
    "    sys.exit()\n",
    "\n",
    "dictionary = retrieveDict(dataset)\n",
    "print('Reading data.')\n",
    "if dataset == 0 or dataset ==  1:\n",
    "    data, labels = readDataFolder(path, dictionary, binaryLabel, rescaleSize)\n",
    "elif dataset == 2:\n",
    "    data, labels = readCSV(path, rescaleSize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualize 5 samples from data.\n",
      "(27455, 64, 64)\n",
      "Gaussian Denoising\n",
      "Canny Edge detection\n",
      "(27455, 64, 64)\n",
      "Not using data augmentation\n",
      "Train on 13452 samples, validate on 5766 samples\n",
      "Epoch 1/100\n",
      "13184/13452 [============================>.] - ETA: 5s - loss: 3.2179 - accuracy: 0.0448"
     ]
    }
   ],
   "source": [
    "#### Main code\n",
    "##Parameters\n",
    "visualize = 1 # if showing images \n",
    "testSize = 0.3 #Percentage of training data kept for testing\n",
    "deepLearning = 1 # 0 for non deep learning methods 1 for CNN\n",
    "\n",
    "## preprocess methods\n",
    "pca = 0 # 1 to do pca, 0 to not do. if doing other preprocessing and augmentation is turned of\n",
    "gammaCorrection = 0 # Do gamma correction to move average pixel intesity to range 100-150 Not implemented yet\n",
    "gaussianBlur = 1 # If we apply gaussian blur to images\n",
    "grayScale = 0 #Convert to grayscale automatically done if using segmentation\n",
    "edgeDetection = 1 # If we do edge detection, using canny edge detector\n",
    "segmentation = 0 # Try to segment image into foreground and background \n",
    "sobelFilter = 0 # Try to segment image into different objects\n",
    "opening = 0 # Do morphological opening, erosion followed by dilation\n",
    "\n",
    "# preprocessing parameters\n",
    "gaussianKernel = 5 #Size of gaussian kernel low pass filter\n",
    "gaussianMean = 0 #Mean of gaussian low pass filter\n",
    "structElementKernel = 3 #Size of structuring element used in opening, erosion etc. \n",
    "cannyKernel = 3 #Kernel for canny edge detection\n",
    "lowThreshold = 40 #Low threshold for canny edge detection\n",
    "highThreshold = 120 #High threshold for canny edge detection\n",
    "\n",
    "## non deep learning classifiers, can only train one model at a time\n",
    "decissionTree = 0\n",
    "crandomForest = 1\n",
    "randomForestMaxDepth = 12\n",
    "kNearestNeighbor = 0\n",
    "quadraticDiscriminantAnalysis = 0\n",
    "linearDiscriminantAnalysis = 0\n",
    "supportVectorClassification = 0 #Slow training\n",
    "\n",
    "# non deep learning data augmentation parameters and methods\n",
    "nonDLAugment = 1 #0 to not augment data, 1 to augment\n",
    "aRuns = 2 #Nr of times to multiply data i.e. 1 leads to twice the original data 2=3 times etc.\n",
    "\n",
    "aNoise = 1 #apply gaussian noise\n",
    "aRotate = 1 # rotate image\n",
    "aFlip = 1 #flip image, horizontaly and vertically\n",
    "\n",
    "aStrength = 0.01 #Strength of gaussian noise\n",
    "aMaxDegree = 90 #Maximum degree to rotate image\n",
    "\n",
    "kfold = 2 #folds for k-fold cross validation used for non deep learning only\n",
    "\n",
    "# cnn architectures\n",
    "bheda = 1 # network from paper \"Using Deep Convolutional Networks for Gesture Recognition in American Sign Language\"\n",
    "kaggle = 0 # network from a kaggle notebook\n",
    "islalm = 0 # network from  paper \"Recognition Bangla Sign Language using Convolutional Neural Network\"\n",
    "\n",
    "# deep learning parameters\n",
    "earlyStoppingPatience = 20 #patience for early stoping i.e how long to continue without increase in accuracy\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "cAugmentation = 0 # 0 to not augment data, 1 to augment\n",
    "\n",
    "if deepLearning == 0:\n",
    "    classifiers = []\n",
    "    classifiers.append(decissionTree)\n",
    "    classifiers.append(crandomForest)\n",
    "    classifiers.append(kNearestNeighbor)\n",
    "    classifiers.append(quadraticDiscriminantAnalysis)\n",
    "    classifiers.append(linearDiscriminantAnalysis)\n",
    "    classifiers.append(supportVectorClassification)\n",
    "    if sum(classifiers) != 1:\n",
    "        print(\"Incorrect number of classifiers selected, program supports training 1 classifier at a time.\")\n",
    "        sys.exit()\n",
    "    aParameter = []\n",
    "    aParameter.append(aRuns)\n",
    "    aParameter.append(aStrength)\n",
    "    aParameter.append(aMaxDegree)\n",
    "    aParameter.append(rescaleSize)\n",
    "\n",
    "    aMethods =[]\n",
    "    aMethods.append(aNoise)\n",
    "    aMethods.append(aRotate)\n",
    "    aMethods.append(aFlip)\n",
    "\n",
    "    #non deep learning classifier parameters\n",
    "    classifierParameters = []\n",
    "    classifierParameters.append(randomForestMaxDepth) \n",
    "\n",
    "if pca == 1:\n",
    "    gammaCorrection = 0 \n",
    "    gaussianBlur = 0 \n",
    "    grayScale = 0 \n",
    "    edgeDetection = 0 \n",
    "    segmentation = 0 \n",
    "    sobelFilter = 0 \n",
    "    opening = 0 \n",
    "    deepLearning = 1\n",
    "    \n",
    "if deepLearning == 1:\n",
    "\n",
    "    cArchitecture = []\n",
    "    cArchitecture.append(bheda)\n",
    "    cArchitecture.append(kaggle)\n",
    "    cArchitecture.append(islalm) \n",
    "    if sum(cArchitecture) != 1:\n",
    "        print(\"Warning: Train one network at a time.\")\n",
    "        sys.exit()\n",
    "    cParameter = []\n",
    "    cParameter.append(earlyStoppingPatience)\n",
    "    cParameter.append(BATCH_SIZE)\n",
    "    cParameter.append(EPOCHS)\n",
    "    cParameter.append(cAugmentation)\n",
    "\n",
    "if dataset == 2:\n",
    "    grayScale = 0 #don't try to convert grayscale image to grayscale\n",
    "\n",
    "preprocessingParameters = []\n",
    "preprocessingParameters.append(gaussianKernel)\n",
    "preprocessingParameters.append(gaussianMean)\n",
    "preprocessingParameters.append(structElementKernel)\n",
    "preprocessingParameters.append(cannyKernel)\n",
    "preprocessingParameters.append(lowThreshold)\n",
    "preprocessingParameters.append(highThreshold)\n",
    "\n",
    "methods = []\n",
    "methods.append(gammaCorrection)\n",
    "methods.append(gaussianBlur)\n",
    "methods.append(grayScale)\n",
    "methods.append(edgeDetection)\n",
    "methods.append(segmentation)\n",
    "methods.append(sobelFilter)\n",
    "methods.append(opening)\n",
    "\n",
    "\n",
    "\n",
    "# Visualize some samples of data\n",
    "if visualize == 1:\n",
    "    print('Visualize 5 samples from data.')\n",
    "    plotSamples(data,5,rescaleSize)\n",
    "print(np.shape(data))\n",
    "## Preprocessing\n",
    "preprocessedData = preprocessing(methods, data, rescaleSize, preprocessingParameters, dataset)\n",
    "print(np.shape(preprocessedData))\n",
    "\n",
    "if deepLearning == 0: #From here and down is specific things for non deep learning classification\n",
    "    if binaryLabel == 1 and (quadraticDiscriminantAnalysis == 1 or linearDiscriminantAnalysis ==1):\n",
    "        print(\"Warning: QDA and LDA does not work with binary labels setting labels to non binary\")\n",
    "        binaryLabel = 0\n",
    "\n",
    "    ## Reshape data into size(nr_samples,features) \n",
    "    if len(np.shape(preprocessedData)) == 4:\n",
    "        preprocessedData = grayscale(preprocessedData)\n",
    "        preprocessedData = preprocessedData.reshape(-1,(rescaleSize*rescaleSize))  \n",
    "    else:\n",
    "        preprocessedData = preprocessedData.reshape(-1,(rescaleSize*rescaleSize))  \n",
    "\n",
    "    # Split data into train and test data and convert labels to binary if desired\n",
    "    trainData, testData, trainLabel, testLabel = splitData(preprocessedData, labels, testSize, binaryLabel, pca)\n",
    "    \n",
    "    # augment data if desired\n",
    "    if nonDLAugment == 1:\n",
    "        trainData, trainLabel = augmentData(trainData, trainLabel, aParameter, aMethods)\n",
    "\n",
    "    ## Train and evaluate non deep learning model\n",
    "    model = trainNonDeepLearning(classifiers, classifierParameters, trainData, trainLabel)\n",
    "    evaluateSKlearn(model,testData,testLabel, kfold, visualize)\n",
    "\n",
    "if deepLearning == 1:\n",
    "    if len(np.shape(preprocessedData)) < 4: #grayscale/binary might come in shape (samples x size x size), but we need (samples x size x size x channels)\n",
    "        preprocessedData = preprocessedData[..., np.newaxis]\n",
    "    \n",
    "    trainData, testData, trainLabel, testLabel = splitData(preprocessedData, labels, testSize, binaryLabel, pca) #not sure if binary labels work with the dimensions\n",
    "    \n",
    "    network = trainCNN(rescaleSize, trainData, trainLabel, cArchitecture, cParameter)\n",
    "\n",
    "    evaluateCNN(testData, testLabel, network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
